{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26b8778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import v2\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torcheval.metrics import Mean\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb495c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train/metadata.csv')\n",
    "test_df = pd.read_csv('test/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f511fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    'blue': 0,\n",
    "    'brown':1,\n",
    "    'red': 2,\n",
    "    'yellow': 3,\n",
    "    'green': 4\n",
    "}\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "bs = 5\n",
    "lamb = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5176f74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AshrafiSet(Dataset):\n",
    "\n",
    "    def __init__(self, data, phase):\n",
    "        self.X = list()\n",
    "        self.gt = list()\n",
    "        self.con = list()\n",
    "        to_tensor = v2.ToTensor()\n",
    "        for i, row in data.iterrows():\n",
    "            if i % 25 == 0:\n",
    "               X = Image.open(f'{phase}/inputs/{row[\"input\"]}')\n",
    "            y_image = Image.open(f'{phase}/targets/{row[\"target\"]}')\n",
    "            hair, shirt = colors[row['hair']], colors[row['shirt']]\n",
    "\n",
    "            self.con.append(torch.LongTensor([hair, shirt]))\n",
    "            self.X.append(to_tensor(X))\n",
    "            self.gt.append(to_tensor(y_image))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        return self.X[ind], self.gt[ind], self.con[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aeb1bd8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vahid/anaconda3/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_set = AshrafiSet(train_df, 'train')\n",
    "test_set = AshrafiSet(test_df, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "487bd261",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89762342",
   "metadata": {},
   "source": [
    "# NeuralNetwork\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a24a46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, in_f, out_f, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_f, out_f, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_f),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(out_f, out_f, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_f),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class UBadNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        # down sample, decoder phase\n",
    "        self.block1 = Block(in_channels, 64)\n",
    "        self.block2 = Block(64, 128)\n",
    "        self.block3 = Block(128, 256)\n",
    "        self.block4 = Block(256, 512)\n",
    "        self.block5 = Block(512, 1024)\n",
    "        self.block6 = Block(1024, 1024)\n",
    "\n",
    "        # Going N_block for concated tensors on decoder\n",
    "        self.n_bloc_1 = Block(1024, 512)\n",
    "        self.n_bloc_2 = Block(512, 256)\n",
    "        self.n_bloc_3 = Block(256, 128)\n",
    "        self.n_bloc_4 = Block(128, 64)\n",
    "\n",
    "        # up sample\n",
    "        self.upc_1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2, )\n",
    "        self.upc_2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.upc_3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.upc_4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "\n",
    "        self.last_layer = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "\n",
    "        self.mp = nn.MaxPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_1 = self.block1(x)  \n",
    "        x_m = self.mp(x_1)  \n",
    "        x_2 = self.block2(x_m) \n",
    "        x_m = self.mp(x_2)  \n",
    "        x_3 = self.block3(x_m) \n",
    "        x_m = self.mp(x_3) \n",
    "        x_4 = self.block4(x_m)  \n",
    "        x_m = self.mp(x_4)  \n",
    "        x_5 = self.block5(x_m) \n",
    "\n",
    "        y_temp = self.relu(self.dropout(self.upc_1(x_5)))  \n",
    "        y = self.n_bloc_1(torch.cat([y_temp, x_4], dim=1))  \n",
    "        y_temp = self.relu(self.dropout(self.upc_2(y)))  \n",
    "        y = self.n_bloc_2(torch.cat([y_temp, x_3], dim=1))  \n",
    "        y_temp = self.relu(self.dropout(self.upc_3(y)))  \n",
    "        y = self.n_bloc_3(torch.cat([y_temp, x_2], dim=1))  \n",
    "        y_temp = self.relu(self.dropout(self.upc_4(y)))  \n",
    "        y = self.n_bloc_4(torch.cat([y_temp, x_1], dim=1))  \n",
    "\n",
    "        y = self.last_layer(y)  \n",
    "        y = self.tanh(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(colors), 128)\n",
    "        self.unet = UBadNet(33)\n",
    "        self.down_sampler = nn.Conv2d(3, 32, kernel_size=3, padding=1, stride=4)\n",
    "        self.up_sampler = nn.Sequential(\n",
    "            nn.ConvTranspose2d(3, 3, kernel_size=2, stride=2),\n",
    "            nn.ConvTranspose2d(3, 3, kernel_size=2, stride=2)\n",
    "            )\n",
    "\n",
    "    def forward(self, x, conditions=None):\n",
    "\n",
    "        conditions = self.embedding(conditions)\n",
    "        labels = conditions.unsqueeze(1).repeat(1, 1, 64, 1)\n",
    "        x = self.down_sampler(x)\n",
    "        x = torch.cat([x, labels], dim=1)\n",
    "        output = self.unet(x)\n",
    "        output = self.up_sampler(output)\n",
    "        return output\n",
    "\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(colors), embedding_dim=128)\n",
    "        block1 = Block(33, 64)\n",
    "        block2 = Block(64, 128)\n",
    "        self.mp = nn.MaxPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        block3 = Block(128, 256)\n",
    "        block4 = Block(256, 512)\n",
    "        block5 = Block(512, 256)\n",
    "        block6 = Block(256, 64)\n",
    "        self.blocks = nn.ModuleList([block1, block2, block3, block4, block5, block6])\n",
    "        self.down_sampler = nn.Conv2d(6, 32, kernel_size=3, padding=1, stride=4)\n",
    "        self.head = nn.Linear(256, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, x, y, conditions):\n",
    "        x = torch.cat([x, y], dim=1)\n",
    "        x = self.down_sampler(x)\n",
    "        conditions = self.embedding(conditions)\n",
    "        conditions = conditions.unsqueeze(1).repeat(1, 1, 64, 1)\n",
    "\n",
    "        x = torch.cat([conditions, x], dim=1)\n",
    "        for module in self.blocks:\n",
    "            x = module(x)\n",
    "            x = self.mp(x)\n",
    "            \n",
    "        x = x.flatten(1)\n",
    "        y = self.head(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a644148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = Generator().to(device)\n",
    "# disc = Discriminator().to(device)\n",
    "gen = torch.load('gen.pt')\n",
    "disc = torch.load('disc.pt')\n",
    "gen_optimizer = torch.optim.Adam(gen.parameters())\n",
    "disc_optimizer = torch.optim.Adam(disc.parameters())\n",
    "l1_loss = nn.L1Loss()\n",
    "disc_loss = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b0cdd",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4708783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(disc, gen, loader, d_loss, l1_fn, lamb, g_optimizer, d_optimizer):\n",
    "    mean_d = Mean().to(device)\n",
    "    mean_g = Mean().to(device)\n",
    "    with tqdm(loader, unit='batch') as tep:\n",
    "        for x, y, cond in tep:\n",
    "            x, y, cond = x.to(device), y.to(device), cond.to(device)\n",
    "            valid = torch.ones([bs, 1]).to(device)\n",
    "            fake = torch.zeros([bs, 1]).to(device)\n",
    "            for i in range(5):\n",
    "                loss_disc_real = d_loss(disc(x, y, cond), valid)\n",
    "                gen_outputs = gen(x, cond)\n",
    "\n",
    "                loss_disc_fake = d_loss(disc(x, gen_outputs, cond), fake)\n",
    "\n",
    "                disc_loss = loss_disc_fake + loss_disc_real\n",
    "                disc_loss.backward()\n",
    "                d_optimizer.step()\n",
    "                d_optimizer.zero_grad()\n",
    "\n",
    "            g_optimizer.zero_grad()\n",
    "\n",
    "            gen_outputs = gen(x, cond)\n",
    "\n",
    "            gen_loss = d_loss(disc(gen_outputs, y, cond), valid)\n",
    "            l1_loss = l1_fn(gen_outputs, y)\n",
    "\n",
    "            total_gen_loss = gen_loss + (lamb * l1_loss)\n",
    "\n",
    "            total_gen_loss.backward()\n",
    "            clip_grad_norm_(gen.parameters(), 0.5)\n",
    "            g_optimizer.step()\n",
    "            mean_d.update(disc_loss)\n",
    "            mean_g.update(total_gen_loss)\n",
    "            \n",
    "            tep.set_postfix(loss_gan=mean_g.compute().item(), loss_disc=mean_d.compute().item())\n",
    "        return disc, gen, mean_g.compute().item(), mean_d.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd2dcdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████| 20/20 [00:07<00:00,  2.59batch/s, loss_disc=1.19e-5, loss_gan=4.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/20 [00:00<?, ?batch/s]WARNING:root:No calls to update() have been made - returning 0.0\n",
      "  5%|▋             | 1/20 [00:00<00:06,  2.87batch/s, loss_disc=0, loss_gan=4.6]WARNING:root:No calls to update() have been made - returning 0.0\n",
      "100%|██████| 20/20 [00:07<00:00,  2.83batch/s, loss_disc=5.72e-8, loss_gan=3.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/20 [00:00<?, ?batch/s]WARNING:root:No calls to update() have been made - returning 0.0\n",
      "  5%|▋            | 1/20 [00:00<00:06,  2.87batch/s, loss_disc=0, loss_gan=3.11]WARNING:root:No calls to update() have been made - returning 0.0\n",
      "100%|██████| 20/20 [00:07<00:00,  2.85batch/s, loss_disc=2.34e-7, loss_gan=3.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/20 [00:00<?, ?batch/s]WARNING:root:No calls to update() have been made - returning 0.0\n",
      "  5%|▋            | 1/20 [00:00<00:06,  2.87batch/s, loss_disc=0, loss_gan=3.07]WARNING:root:No calls to update() have been made - returning 0.0\n",
      " 10%|█▎           | 2/20 [00:00<00:06,  2.87batch/s, loss_disc=0, loss_gan=3.21]WARNING:root:No calls to update() have been made - returning 0.0\n",
      " 15%|█▉           | 3/20 [00:01<00:05,  2.86batch/s, loss_disc=0, loss_gan=3.31]WARNING:root:No calls to update() have been made - returning 0.0\n",
      "100%|██████| 20/20 [00:06<00:00,  2.87batch/s, loss_disc=3.93e-8, loss_gan=3.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/20 [00:00<?, ?batch/s]WARNING:root:No calls to update() have been made - returning 0.0\n",
      "  5%|▋            | 1/20 [00:00<00:06,  2.86batch/s, loss_disc=0, loss_gan=3.49]WARNING:root:No calls to update() have been made - returning 0.0\n",
      " 95%|█████▋| 19/20 [00:07<00:00,  2.68batch/s, loss_disc=1.25e-9, loss_gan=3.46]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m         disc, gen, loss_g, loss_d \u001b[38;5;241m=\u001b[39m train_one_epoch(disc, \n\u001b[1;32m      3\u001b[0m                                                     gen, \n\u001b[1;32m      4\u001b[0m                                                     train_loader, \n\u001b[1;32m      5\u001b[0m                                                     disc_loss,\n\u001b[1;32m      6\u001b[0m                                                     l1_loss, \n\u001b[1;32m      7\u001b[0m                                                     lamb, \n\u001b[1;32m      8\u001b[0m                                                     gen_optimizer, \n\u001b[1;32m      9\u001b[0m                                                     disc_optimizer)\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 35\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(disc, gen, loader, d_loss, l1_fn, lamb, g_optimizer, d_optimizer)\u001b[0m\n\u001b[1;32m     32\u001b[0m     mean_d\u001b[38;5;241m.\u001b[39mupdate(disc_loss)\n\u001b[1;32m     33\u001b[0m     mean_g\u001b[38;5;241m.\u001b[39mupdate(total_gen_loss)\n\u001b[0;32m---> 35\u001b[0m     tep\u001b[38;5;241m.\u001b[39mset_postfix(loss_gan\u001b[38;5;241m=\u001b[39mmean_g\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;241m.\u001b[39mitem(), loss_disc\u001b[38;5;241m=\u001b[39mmean_d\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m disc, gen, mean_g\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;241m.\u001b[39mitem(), mean_d\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "        disc, gen, loss_g, loss_d = train_one_epoch(disc, \n",
    "                                                    gen, \n",
    "                                                    train_loader, \n",
    "                                                    disc_loss,\n",
    "                                                    l1_loss, \n",
    "                                                    lamb, \n",
    "                                                    gen_optimizer, \n",
    "                                                    disc_optimizer)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6009ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(x, y):\n",
    "    fig = plt.figure(figsize=(15, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title('generated Image')\n",
    "    plt.imshow(x.permute(1, 2, 0))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title('real Image')\n",
    "\n",
    "    plt.imshow(y.permute(1, 2, 0))\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13096d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
